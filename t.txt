kafka1-1     |  inter.broker.listener.name = PLAINTEXT
kafka2-1     |  fetch.max.bytes = 57671680
kafka1-1     |  inter.broker.protocol.version = 3.8-IV0
kafka2-1     |  fetch.purgatory.purge.interval.requests = 1000
kafka1-1     |  kafka.metrics.polling.interval.secs = 10
kafka2-1     |  group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
kafka1-1     |  kafka.metrics.reporters = []
kafka2-1     |  group.consumer.heartbeat.interval.ms = 5000
kafka1-1     |  leader.imbalance.check.interval.seconds = 300
kafka2-1     |  group.consumer.max.heartbeat.interval.ms = 15000
kafka1-1     |  leader.imbalance.per.broker.percentage = 10
kafka2-1     |  group.consumer.max.session.timeout.ms = 60000
kafka1-1     |  listener.security.protocol.map = PLAINTEXT:PLAINTEXT
kafka2-1     |  group.consumer.max.size = 2147483647
kafka1-1     |  listeners = PLAINTEXT://0.0.0.0:9092
kafka2-1     |  group.consumer.migration.policy = disabled
kafka1-1     |  log.cleaner.backoff.ms = 15000
kafka2-1     |  group.consumer.min.heartbeat.interval.ms = 5000
kafka1-1     |  log.cleaner.dedupe.buffer.size = 134217728
kafka2-1     |  group.consumer.min.session.timeout.ms = 45000
kafka1-1     |  log.cleaner.delete.retention.ms = 86400000
kafka2-1     |  group.consumer.session.timeout.ms = 45000
kafka1-1     |  log.cleaner.enable = true
kafka2-1     |  group.coordinator.append.linger.ms = 10
kafka1-1     |  log.cleaner.io.buffer.load.factor = 0.9
kafka2-1     |  group.coordinator.new.enable = false
kafka1-1     |  log.cleaner.io.buffer.size = 524288
kafka2-1     |  group.coordinator.rebalance.protocols = [classic]
kafka1-1     |  log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
kafka2-1     |  group.coordinator.threads = 1
kafka1-1     |  log.cleaner.max.compaction.lag.ms = 9223372036854775807
kafka2-1     |  group.initial.rebalance.delay.ms = 3000
kafka1-1     |  log.cleaner.min.cleanable.ratio = 0.5
kafka2-1     |  group.max.session.timeout.ms = 1800000
kafka1-1     |  log.cleaner.min.compaction.lag.ms = 0
kafka2-1     |  group.max.size = 2147483647
kafka1-1     |  log.cleaner.threads = 1
kafka2-1     |  group.min.session.timeout.ms = 6000
kafka1-1     |  log.cleanup.policy = [delete]
kafka2-1     |  initial.broker.registration.timeout.ms = 60000
kafka1-1     |  log.dir = /tmp/kafka-logs
kafka2-1     |  inter.broker.listener.name = PLAINTEXT
kafka1-1     |  log.dir.failure.timeout.ms = 30000
kafka2-1     |  inter.broker.protocol.version = 3.8-IV0
kafka1-1     |  log.dirs = /var/lib/kafka/data
kafka2-1     |  kafka.metrics.polling.interval.secs = 10
kafka1-1     |  log.flush.interval.messages = 9223372036854775807
kafka2-1     |  kafka.metrics.reporters = []
kafka1-1     |  log.flush.interval.ms = null
kafka2-1     |  leader.imbalance.check.interval.seconds = 300
kafka1-1     |  log.flush.offset.checkpoint.interval.ms = 60000
kafka2-1     |  leader.imbalance.per.broker.percentage = 10
kafka1-1     |  log.flush.scheduler.interval.ms = 9223372036854775807
kafka2-1     |  listener.security.protocol.map = PLAINTEXT:PLAINTEXT
kafka1-1     |  log.flush.start.offset.checkpoint.interval.ms = 60000
kafka2-1     |  listeners = PLAINTEXT://0.0.0.0:9093
kafka1-1     |  log.index.interval.bytes = 4096
kafka2-1     |  log.cleaner.backoff.ms = 15000
kafka1-1     |  log.index.size.max.bytes = 10485760
kafka2-1     |  log.cleaner.dedupe.buffer.size = 134217728
kafka1-1     |  log.initial.task.delay.ms = 30000
kafka2-1     |  log.cleaner.delete.retention.ms = 86400000
kafka1-1     |  log.local.retention.bytes = -2
kafka2-1     |  log.cleaner.enable = true
kafka1-1     |  log.local.retention.ms = -2
kafka2-1     |  log.cleaner.io.buffer.load.factor = 0.9
kafka1-1     |  log.message.downconversion.enable = true
kafka2-1     |  log.cleaner.io.buffer.size = 524288
kafka1-1     |  log.message.format.version = 3.0-IV1
kafka2-1     |  log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
kafka1-1     |  log.message.timestamp.after.max.ms = 9223372036854775807
kafka2-1     |  log.cleaner.max.compaction.lag.ms = 9223372036854775807
kafka1-1     |  log.message.timestamp.before.max.ms = 9223372036854775807
kafka2-1     |  log.cleaner.min.cleanable.ratio = 0.5
kafka1-1     |  log.message.timestamp.difference.max.ms = 9223372036854775807
kafka2-1     |  log.cleaner.min.compaction.lag.ms = 0
kafka1-1     |  log.message.timestamp.type = CreateTime
kafka2-1     |  log.cleaner.threads = 1
kafka1-1     |  log.preallocate = false
kafka2-1     |  log.cleanup.policy = [delete]
kafka1-1     |  log.retention.bytes = -1
kafka2-1     |  log.dir = /tmp/kafka-logs
kafka1-1     |  log.retention.check.interval.ms = 300000
kafka2-1     |  log.dir.failure.timeout.ms = 30000
kafka1-1     |  log.retention.hours = 168
kafka2-1     |  log.dirs = /var/lib/kafka/data
kafka1-1     |  log.retention.minutes = null
kafka2-1     |  log.flush.interval.messages = 9223372036854775807
kafka1-1     |  log.retention.ms = null
kafka2-1     |  log.flush.interval.ms = null
kafka1-1     |  log.roll.hours = 168
kafka2-1     |  log.flush.offset.checkpoint.interval.ms = 60000
kafka1-1     |  log.roll.jitter.hours = 0
kafka2-1     |  log.flush.scheduler.interval.ms = 9223372036854775807
kafka1-1     |  log.roll.jitter.ms = null
kafka2-1     |  log.flush.start.offset.checkpoint.interval.ms = 60000
kafka1-1     |  log.roll.ms = null
kafka2-1     |  log.index.interval.bytes = 4096
kafka1-1     |  log.segment.bytes = 1073741824
kafka2-1     |  log.index.size.max.bytes = 10485760
kafka1-1     |  log.segment.delete.delay.ms = 60000
kafka2-1     |  log.initial.task.delay.ms = 30000
kafka1-1     |  max.connection.creation.rate = 2147483647
kafka2-1     |  log.local.retention.bytes = -2
kafka1-1     |  max.connections = 2147483647
kafka2-1     |  log.local.retention.ms = -2
kafka1-1     |  max.connections.per.ip = 2147483647
kafka2-1     |  log.message.downconversion.enable = true
kafka1-1     |  max.connections.per.ip.overrides =
kafka2-1     |  log.message.format.version = 3.0-IV1
kafka1-1     |  max.incremental.fetch.session.cache.slots = 1000
kafka2-1     |  log.message.timestamp.after.max.ms = 9223372036854775807
kafka1-1     |  max.request.partition.size.limit = 2000
kafka2-1     |  log.message.timestamp.before.max.ms = 9223372036854775807
kafka1-1     |  message.max.bytes = 1048588
kafka2-1     |  log.message.timestamp.difference.max.ms = 9223372036854775807
kafka1-1     |  metadata.log.dir = null
kafka2-1     |  log.message.timestamp.type = CreateTime
kafka1-1     |  metadata.log.max.record.bytes.between.snapshots = 20971520
kafka2-1     |  log.preallocate = false
kafka1-1     |  metadata.log.max.snapshot.interval.ms = 3600000
kafka2-1     |  log.retention.bytes = -1
kafka1-1     |  metadata.log.segment.bytes = 1073741824
kafka2-1     |  log.retention.check.interval.ms = 300000
kafka1-1     |  metadata.log.segment.min.bytes = 8388608
kafka2-1     |  log.retention.hours = 168
kafka1-1     |  metadata.log.segment.ms = 604800000
kafka2-1     |  log.retention.minutes = null
kafka1-1     |  metadata.max.idle.interval.ms = 500
kafka2-1     |  log.retention.ms = null
kafka1-1     |  metadata.max.retention.bytes = 104857600
kafka2-1     |  log.roll.hours = 168
kafka1-1     |  metadata.max.retention.ms = 604800000
kafka2-1     |  log.roll.jitter.hours = 0
kafka1-1     |  metric.reporters = []
kafka2-1     |  log.roll.jitter.ms = null
kafka1-1     |  metrics.num.samples = 2
kafka2-1     |  log.roll.ms = null
kafka1-1     |  metrics.recording.level = INFO
kafka2-1     |  log.segment.bytes = 1073741824
kafka1-1     |  metrics.sample.window.ms = 30000
kafka2-1     |  log.segment.delete.delay.ms = 60000
kafka1-1     |  min.insync.replicas = 1
kafka2-1     |  max.connection.creation.rate = 2147483647
kafka1-1     |  node.id = 1
kafka2-1     |  max.connections = 2147483647
kafka1-1     |  num.io.threads = 8
kafka2-1     |  max.connections.per.ip = 2147483647
kafka1-1     |  num.network.threads = 3
kafka2-1     |  max.connections.per.ip.overrides =
kafka1-1     |  num.partitions = 1
kafka2-1     |  max.incremental.fetch.session.cache.slots = 1000
kafka1-1     |  num.recovery.threads.per.data.dir = 1
kafka2-1     |  max.request.partition.size.limit = 2000
kafka1-1     |  num.replica.alter.log.dirs.threads = null
kafka2-1     |  message.max.bytes = 1048588
kafka1-1     |  num.replica.fetchers = 1
kafka2-1     |  metadata.log.dir = null
kafka1-1     |  offset.metadata.max.bytes = 4096
kafka2-1     |  metadata.log.max.record.bytes.between.snapshots = 20971520
kafka1-1     |  offsets.commit.required.acks = -1
kafka2-1     |  metadata.log.max.snapshot.interval.ms = 3600000
kafka1-1     |  offsets.commit.timeout.ms = 5000
kafka2-1     |  metadata.log.segment.bytes = 1073741824
kafka1-1     |  offsets.load.buffer.size = 5242880
kafka2-1     |  metadata.log.segment.min.bytes = 8388608
kafka1-1     |  offsets.retention.check.interval.ms = 600000
kafka2-1     |  metadata.log.segment.ms = 604800000
kafka1-1     |  offsets.retention.minutes = 10080
kafka2-1     |  metadata.max.idle.interval.ms = 500
kafka1-1     |  offsets.topic.compression.codec = 0
kafka2-1     |  metadata.max.retention.bytes = 104857600
kafka1-1     |  offsets.topic.num.partitions = 50
kafka2-1     |  metadata.max.retention.ms = 604800000
kafka1-1     |  offsets.topic.replication.factor = 3
kafka2-1     |  metric.reporters = []
kafka1-1     |  offsets.topic.segment.bytes = 104857600
kafka2-1     |  metrics.num.samples = 2
kafka1-1     |  password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
kafka2-1     |  metrics.recording.level = INFO
kafka1-1     |  password.encoder.iterations = 4096
kafka2-1     |  metrics.sample.window.ms = 30000
kafka1-1     |  password.encoder.key.length = 128
kafka2-1     |  min.insync.replicas = 1
kafka1-1     |  password.encoder.keyfactory.algorithm = null
kafka2-1     |  node.id = 2
kafka1-1     |  password.encoder.old.secret = null
kafka2-1     |  num.io.threads = 8
kafka1-1     |  password.encoder.secret = null
kafka2-1     |  num.network.threads = 3
kafka1-1     |  principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
kafka2-1     |  num.partitions = 1
kafka1-1     |  process.roles = []
kafka2-1     |  num.recovery.threads.per.data.dir = 1
kafka1-1     |  producer.id.expiration.check.interval.ms = 600000
kafka2-1     |  num.replica.alter.log.dirs.threads = null
kafka1-1     |  producer.id.expiration.ms = 86400000
kafka2-1     |  num.replica.fetchers = 1
kafka1-1     |  producer.purgatory.purge.interval.requests = 1000
kafka2-1     |  offset.metadata.max.bytes = 4096
kafka1-1     |  queued.max.request.bytes = -1
kafka2-1     |  offsets.commit.required.acks = -1
kafka1-1     |  queued.max.requests = 500
kafka2-1     |  offsets.commit.timeout.ms = 5000
kafka1-1     |  quota.window.num = 11
kafka2-1     |  offsets.load.buffer.size = 5242880
kafka1-1     |  quota.window.size.seconds = 1
kafka2-1     |  offsets.retention.check.interval.ms = 600000
kafka1-1     |  remote.fetch.max.wait.ms = 500
kafka2-1     |  offsets.retention.minutes = 10080
kafka1-1     |  remote.log.index.file.cache.total.size.bytes = 1073741824
kafka2-1     |  offsets.topic.compression.codec = 0
kafka1-1     |  remote.log.manager.copier.thread.pool.size = 10
kafka2-1     |  offsets.topic.num.partitions = 50
kafka1-1     |  remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
kafka2-1     |  offsets.topic.replication.factor = 3
kafka1-1     |  remote.log.manager.copy.quota.window.num = 11
kafka2-1     |  offsets.topic.segment.bytes = 104857600
kafka1-1     |  remote.log.manager.copy.quota.window.size.seconds = 1
kafka2-1     |  password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
kafka1-1     |  remote.log.manager.expiration.thread.pool.size = 10
kafka2-1     |  password.encoder.iterations = 4096
kafka1-1     |  remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
kafka2-1     |  password.encoder.key.length = 128
kafka1-1     |  remote.log.manager.fetch.quota.window.num = 11
kafka2-1     |  password.encoder.keyfactory.algorithm = null
kafka1-1     |  remote.log.manager.fetch.quota.window.size.seconds = 1
kafka2-1     |  password.encoder.old.secret = null
kafka1-1     |  remote.log.manager.task.interval.ms = 30000
kafka2-1     |  password.encoder.secret = null
kafka1-1     |  remote.log.manager.task.retry.backoff.max.ms = 30000
kafka2-1     |  principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
kafka1-1     |  remote.log.manager.task.retry.backoff.ms = 500
kafka2-1     |  process.roles = []
kafka1-1     |  remote.log.manager.task.retry.jitter = 0.2
kafka2-1     |  producer.id.expiration.check.interval.ms = 600000
kafka1-1     |  remote.log.manager.thread.pool.size = 10
kafka2-1     |  producer.id.expiration.ms = 86400000
kafka1-1     |  remote.log.metadata.custom.metadata.max.bytes = 128
kafka2-1     |  producer.purgatory.purge.interval.requests = 1000
kafka1-1     |  remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
kafka2-1     |  queued.max.request.bytes = -1
kafka1-1     |  remote.log.metadata.manager.class.path = null
kafka2-1     |  queued.max.requests = 500
kafka1-1     |  remote.log.metadata.manager.impl.prefix = rlmm.config.
kafka2-1     |  quota.window.num = 11
kafka1-1     |  remote.log.metadata.manager.listener.name = null
kafka2-1     |  quota.window.size.seconds = 1
kafka1-1     |  remote.log.reader.max.pending.tasks = 100
kafka2-1     |  remote.fetch.max.wait.ms = 500
kafka1-1     |  remote.log.reader.threads = 10
kafka2-1     |  remote.log.index.file.cache.total.size.bytes = 1073741824
kafka1-1     |  remote.log.storage.manager.class.name = null
kafka2-1     |  remote.log.manager.copier.thread.pool.size = 10
kafka1-1     |  remote.log.storage.manager.class.path = null
kafka2-1     |  remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
kafka1-1     |  remote.log.storage.manager.impl.prefix = rsm.config.
kafka2-1     |  remote.log.manager.copy.quota.window.num = 11
kafka1-1     |  remote.log.storage.system.enable = false
kafka2-1     |  remote.log.manager.copy.quota.window.size.seconds = 1
kafka1-1     |  replica.fetch.backoff.ms = 1000
kafka2-1     |  remote.log.manager.expiration.thread.pool.size = 10
kafka1-1     |  replica.fetch.max.bytes = 1048576
kafka2-1     |  remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
kafka1-1     |  replica.fetch.min.bytes = 1
kafka2-1     |  remote.log.manager.fetch.quota.window.num = 11
kafka1-1     |  replica.fetch.response.max.bytes = 10485760
kafka2-1     |  remote.log.manager.fetch.quota.window.size.seconds = 1
kafka1-1     |  replica.fetch.wait.max.ms = 500
kafka2-1     |  remote.log.manager.task.interval.ms = 30000
kafka1-1     |  replica.high.watermark.checkpoint.interval.ms = 5000
kafka2-1     |  remote.log.manager.task.retry.backoff.max.ms = 30000
kafka1-1     |  replica.lag.time.max.ms = 30000
kafka2-1     |  remote.log.manager.task.retry.backoff.ms = 500
kafka1-1     |  replica.selector.class = null
kafka2-1     |  remote.log.manager.task.retry.jitter = 0.2
kafka1-1     |  replica.socket.receive.buffer.bytes = 65536
kafka2-1     |  remote.log.manager.thread.pool.size = 10
kafka1-1     |  replica.socket.timeout.ms = 30000
kafka2-1     |  remote.log.metadata.custom.metadata.max.bytes = 128
kafka1-1     |  replication.quota.window.num = 11
kafka2-1     |  remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
kafka1-1     |  replication.quota.window.size.seconds = 1
kafka2-1     |  remote.log.metadata.manager.class.path = null
kafka1-1     |  request.timeout.ms = 30000
kafka2-1     |  remote.log.metadata.manager.impl.prefix = rlmm.config.
kafka1-1     |  reserved.broker.max.id = 1000
kafka2-1     |  remote.log.metadata.manager.listener.name = null
kafka1-1     |  sasl.client.callback.handler.class = null
kafka2-1     |  remote.log.reader.max.pending.tasks = 100
kafka1-1     |  sasl.enabled.mechanisms = [GSSAPI]
kafka2-1     |  remote.log.reader.threads = 10
kafka1-1     |  sasl.jaas.config = null
kafka2-1     |  remote.log.storage.manager.class.name = null
kafka1-1     |  sasl.kerberos.kinit.cmd = /usr/bin/kinit
kafka2-1     |  remote.log.storage.manager.class.path = null
kafka1-1     |  sasl.kerberos.min.time.before.relogin = 60000
kafka2-1     |  remote.log.storage.manager.impl.prefix = rsm.config.
kafka1-1     |  sasl.kerberos.principal.to.local.rules = [DEFAULT]
kafka2-1     |  remote.log.storage.system.enable = false
kafka1-1     |  sasl.kerberos.service.name = null
kafka2-1     |  replica.fetch.backoff.ms = 1000
kafka1-1     |  sasl.kerberos.ticket.renew.jitter = 0.05
kafka2-1     |  replica.fetch.max.bytes = 1048576
kafka1-1     |  sasl.kerberos.ticket.renew.window.factor = 0.8
kafka2-1     |  replica.fetch.min.bytes = 1
kafka1-1     |  sasl.login.callback.handler.class = null
kafka2-1     |  replica.fetch.response.max.bytes = 10485760
kafka1-1     |  sasl.login.class = null
kafka2-1     |  replica.fetch.wait.max.ms = 500
kafka1-1     |  sasl.login.connect.timeout.ms = null
kafka2-1     |  replica.high.watermark.checkpoint.interval.ms = 5000
kafka1-1     |  sasl.login.read.timeout.ms = null
kafka2-1     |  replica.lag.time.max.ms = 30000
kafka1-1     |  sasl.login.refresh.buffer.seconds = 300
kafka2-1     |  replica.selector.class = null
kafka1-1     |  sasl.login.refresh.min.period.seconds = 60
kafka2-1     |  replica.socket.receive.buffer.bytes = 65536
kafka1-1     |  sasl.login.refresh.window.factor = 0.8
kafka2-1     |  replica.socket.timeout.ms = 30000
kafka1-1     |  sasl.login.refresh.window.jitter = 0.05
kafka2-1     |  replication.quota.window.num = 11
kafka1-1     |  sasl.login.retry.backoff.max.ms = 10000
kafka2-1     |  replication.quota.window.size.seconds = 1
kafka1-1     |  sasl.login.retry.backoff.ms = 100
kafka2-1     |  request.timeout.ms = 30000
kafka1-1     |  sasl.mechanism.controller.protocol = GSSAPI
kafka2-1     |  reserved.broker.max.id = 1000
kafka1-1     |  sasl.mechanism.inter.broker.protocol = GSSAPI
kafka2-1     |  sasl.client.callback.handler.class = null
kafka1-1     |  sasl.oauthbearer.clock.skew.seconds = 30
kafka2-1     |  sasl.enabled.mechanisms = [GSSAPI]
kafka1-1     |  sasl.oauthbearer.expected.audience = null
kafka2-1     |  sasl.jaas.config = null
kafka1-1     |  sasl.oauthbearer.expected.issuer = null
kafka2-1     |  sasl.kerberos.kinit.cmd = /usr/bin/kinit
kafka1-1     |  sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
kafka2-1     |  sasl.kerberos.min.time.before.relogin = 60000
kafka1-1     |  sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
kafka2-1     |  sasl.kerberos.principal.to.local.rules = [DEFAULT]
kafka1-1     |  sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
kafka2-1     |  sasl.kerberos.service.name = null
kafka1-1     |  sasl.oauthbearer.jwks.endpoint.url = null
kafka2-1     |  sasl.kerberos.ticket.renew.jitter = 0.05
kafka1-1     |  sasl.oauthbearer.scope.claim.name = scope
kafka2-1     |  sasl.kerberos.ticket.renew.window.factor = 0.8
kafka1-1     |  sasl.oauthbearer.sub.claim.name = sub
kafka2-1     |  sasl.login.callback.handler.class = null
kafka1-1     |  sasl.oauthbearer.token.endpoint.url = null
kafka2-1     |  sasl.login.class = null
kafka1-1     |  sasl.server.callback.handler.class = null
kafka2-1     |  sasl.login.connect.timeout.ms = null
kafka1-1     |  sasl.server.max.receive.size = 524288
kafka2-1     |  sasl.login.read.timeout.ms = null
kafka1-1     |  security.inter.broker.protocol = PLAINTEXT
kafka2-1     |  sasl.login.refresh.buffer.seconds = 300
kafka1-1     |  security.providers = null
kafka2-1     |  sasl.login.refresh.min.period.seconds = 60
kafka1-1     |  server.max.startup.time.ms = 9223372036854775807
kafka2-1     |  sasl.login.refresh.window.factor = 0.8
kafka1-1     |  socket.connection.setup.timeout.max.ms = 30000
kafka2-1     |  sasl.login.refresh.window.jitter = 0.05
kafka1-1     |  socket.connection.setup.timeout.ms = 10000
kafka2-1     |  sasl.login.retry.backoff.max.ms = 10000
kafka1-1     |  socket.listen.backlog.size = 50
kafka2-1     |  sasl.login.retry.backoff.ms = 100
kafka1-1     |  socket.receive.buffer.bytes = 102400
kafka2-1     |  sasl.mechanism.controller.protocol = GSSAPI
kafka1-1     |  socket.request.max.bytes = 104857600
kafka2-1     |  sasl.mechanism.inter.broker.protocol = GSSAPI
kafka1-1     |  socket.send.buffer.bytes = 102400
kafka2-1     |  sasl.oauthbearer.clock.skew.seconds = 30
kafka1-1     |  ssl.allow.dn.changes = false
kafka2-1     |  sasl.oauthbearer.expected.audience = null
kafka1-1     |  ssl.allow.san.changes = false
kafka2-1     |  sasl.oauthbearer.expected.issuer = null
kafka1-1     |  ssl.cipher.suites = []
kafka2-1     |  sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
kafka1-1     |  ssl.client.auth = none
kafka2-1     |  sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
kafka1-1     |  ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
kafka2-1     |  sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
kafka1-1     |  ssl.endpoint.identification.algorithm = https
kafka2-1     |  sasl.oauthbearer.jwks.endpoint.url = null
kafka1-1     |  ssl.engine.factory.class = null
kafka2-1     |  sasl.oauthbearer.scope.claim.name = scope
kafka1-1     |  ssl.key.password = null
kafka2-1     |  sasl.oauthbearer.sub.claim.name = sub
kafka1-1     |  ssl.keymanager.algorithm = SunX509
kafka2-1     |  sasl.oauthbearer.token.endpoint.url = null
kafka1-1     |  ssl.keystore.certificate.chain = null
kafka2-1     |  sasl.server.callback.handler.class = null
kafka1-1     |  ssl.keystore.key = null
kafka2-1     |  sasl.server.max.receive.size = 524288
kafka1-1     |  ssl.keystore.location = null
kafka2-1     |  security.inter.broker.protocol = PLAINTEXT
kafka1-1     |  ssl.keystore.password = null
kafka2-1     |  security.providers = null
kafka1-1     |  ssl.keystore.type = JKS
kafka2-1     |  server.max.startup.time.ms = 9223372036854775807
kafka1-1     |  ssl.principal.mapping.rules = DEFAULT
kafka2-1     |  socket.connection.setup.timeout.max.ms = 30000
kafka1-1     |  ssl.protocol = TLSv1.3
kafka2-1     |  socket.connection.setup.timeout.ms = 10000
kafka1-1     |  ssl.provider = null
kafka2-1     |  socket.listen.backlog.size = 50
kafka1-1     |  ssl.secure.random.implementation = null
kafka2-1     |  socket.receive.buffer.bytes = 102400
kafka1-1     |  ssl.trustmanager.algorithm = PKIX
kafka2-1     |  socket.request.max.bytes = 104857600
kafka1-1     |  ssl.truststore.certificates = null
kafka2-1     |  socket.send.buffer.bytes = 102400
kafka1-1     |  ssl.truststore.location = null
kafka2-1     |  ssl.allow.dn.changes = false
kafka1-1     |  ssl.truststore.password = null
kafka2-1     |  ssl.allow.san.changes = false
kafka1-1     |  ssl.truststore.type = JKS
kafka2-1     |  ssl.cipher.suites = []
kafka1-1     |  telemetry.max.bytes = 1048576
kafka2-1     |  ssl.client.auth = none
kafka1-1     |  transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
kafka2-1     |  ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
kafka1-1     |  transaction.max.timeout.ms = 900000
kafka2-1     |  ssl.endpoint.identification.algorithm = https
kafka1-1     |  transaction.partition.verification.enable = true
kafka2-1     |  ssl.engine.factory.class = null
kafka1-1     |  transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
kafka2-1     |  ssl.key.password = null
kafka1-1     |  transaction.state.log.load.buffer.size = 5242880
kafka2-1     |  ssl.keymanager.algorithm = SunX509
kafka1-1     |  transaction.state.log.min.isr = 2
kafka2-1     |  ssl.keystore.certificate.chain = null
kafka1-1     |  transaction.state.log.num.partitions = 50
kafka2-1     |  ssl.keystore.key = null
kafka1-1     |  transaction.state.log.replication.factor = 3
kafka2-1     |  ssl.keystore.location = null
kafka1-1     |  transaction.state.log.segment.bytes = 104857600
kafka2-1     |  ssl.keystore.password = null
kafka1-1     |  transactional.id.expiration.ms = 604800000
kafka2-1     |  ssl.keystore.type = JKS
kafka1-1     |  unclean.leader.election.enable = false
kafka2-1     |  ssl.principal.mapping.rules = DEFAULT
kafka1-1     |  unstable.api.versions.enable = false
kafka2-1     |  ssl.protocol = TLSv1.3
kafka1-1     |  unstable.feature.versions.enable = false
kafka2-1     |  ssl.provider = null
kafka1-1     |  zookeeper.clientCnxnSocket = null
kafka2-1     |  ssl.secure.random.implementation = null
kafka1-1     |  zookeeper.connect = zookeeper:2181
kafka2-1     |  ssl.trustmanager.algorithm = PKIX
kafka1-1     |  zookeeper.connection.timeout.ms = null
kafka2-1     |  ssl.truststore.certificates = null
kafka1-1     |  zookeeper.max.in.flight.requests = 10
kafka2-1     |  ssl.truststore.location = null
kafka1-1     |  zookeeper.metadata.migration.enable = false
kafka2-1     |  ssl.truststore.password = null
kafka1-1     |  zookeeper.metadata.migration.min.batch.size = 200
kafka2-1     |  ssl.truststore.type = JKS
kafka1-1     |  zookeeper.session.timeout.ms = 18000
kafka2-1     |  telemetry.max.bytes = 1048576
kafka1-1     |  zookeeper.set.acl = false
kafka2-1     |  transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
kafka1-1     |  zookeeper.ssl.cipher.suites = null
kafka2-1     |  transaction.max.timeout.ms = 900000
kafka1-1     |  zookeeper.ssl.client.enable = false
kafka2-1     |  transaction.partition.verification.enable = true
kafka1-1     |  zookeeper.ssl.crl.enable = false
kafka2-1     |  transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
kafka1-1     |  zookeeper.ssl.enabled.protocols = null
kafka2-1     |  transaction.state.log.load.buffer.size = 5242880
kafka1-1     |  zookeeper.ssl.endpoint.identification.algorithm = HTTPS
kafka2-1     |  transaction.state.log.min.isr = 2
kafka1-1     |  zookeeper.ssl.keystore.location = null
kafka2-1     |  transaction.state.log.num.partitions = 50
kafka1-1     |  zookeeper.ssl.keystore.password = null
kafka2-1     |  transaction.state.log.replication.factor = 3
kafka1-1     |  zookeeper.ssl.keystore.type = null
kafka2-1     |  transaction.state.log.segment.bytes = 104857600
kafka1-1     |  zookeeper.ssl.ocsp.enable = false
kafka2-1     |  transactional.id.expiration.ms = 604800000
kafka1-1     |  zookeeper.ssl.protocol = TLSv1.2
kafka2-1     |  unclean.leader.election.enable = false
kafka1-1     |  zookeeper.ssl.truststore.location = null
kafka2-1     |  unstable.api.versions.enable = false
kafka1-1     |  zookeeper.ssl.truststore.password = null
kafka2-1     |  unstable.feature.versions.enable = false
kafka1-1     |  zookeeper.ssl.truststore.type = null
kafka2-1     |  zookeeper.clientCnxnSocket = null
kafka1-1     |  (kafka.server.KafkaConfig)
kafka2-1     |  zookeeper.connect = zookeeper:2181
kafka1-1     | [2025-01-02 21:09:47,927] INFO RemoteLogManagerConfig values:
kafka2-1     |  zookeeper.connection.timeout.ms = null
kafka1-1     |  log.local.retention.bytes = -2
kafka2-1     |  zookeeper.max.in.flight.requests = 10
kafka1-1     |  log.local.retention.ms = -2
kafka2-1     |  zookeeper.metadata.migration.enable = false
kafka1-1     |  remote.fetch.max.wait.ms = 500
kafka2-1     |  zookeeper.metadata.migration.min.batch.size = 200
kafka1-1     |  remote.log.index.file.cache.total.size.bytes = 1073741824
kafka2-1     |  zookeeper.session.timeout.ms = 18000
kafka1-1     |  remote.log.manager.copier.thread.pool.size = 10
kafka2-1     |  zookeeper.set.acl = false
kafka1-1     |  remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
kafka2-1     |  zookeeper.ssl.cipher.suites = null
kafka1-1     |  remote.log.manager.copy.quota.window.num = 11
kafka2-1     |  zookeeper.ssl.client.enable = false
kafka1-1     |  remote.log.manager.copy.quota.window.size.seconds = 1
kafka2-1     |  zookeeper.ssl.crl.enable = false
kafka1-1     |  remote.log.manager.expiration.thread.pool.size = 10
kafka2-1     |  zookeeper.ssl.enabled.protocols = null
kafka1-1     |  remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
kafka2-1     |  zookeeper.ssl.endpoint.identification.algorithm = HTTPS
kafka1-1     |  remote.log.manager.fetch.quota.window.num = 11
kafka2-1     |  zookeeper.ssl.keystore.location = null
kafka1-1     |  remote.log.manager.fetch.quota.window.size.seconds = 1
kafka2-1     |  zookeeper.ssl.keystore.password = null
kafka1-1     |  remote.log.manager.task.interval.ms = 30000
kafka2-1     |  zookeeper.ssl.keystore.type = null
kafka1-1     |  remote.log.manager.task.retry.backoff.max.ms = 30000
kafka2-1     |  zookeeper.ssl.ocsp.enable = false
kafka1-1     |  remote.log.manager.task.retry.backoff.ms = 500
kafka2-1     |  zookeeper.ssl.protocol = TLSv1.2
kafka1-1     |  remote.log.manager.task.retry.jitter = 0.2
kafka2-1     |  zookeeper.ssl.truststore.location = null
kafka1-1     |  remote.log.manager.thread.pool.size = 10
kafka2-1     |  zookeeper.ssl.truststore.password = null
kafka1-1     |  remote.log.metadata.custom.metadata.max.bytes = 128
kafka2-1     |  zookeeper.ssl.truststore.type = null
kafka1-1     |  remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
kafka2-1     |  (kafka.server.KafkaConfig)
kafka1-1     |  remote.log.metadata.manager.class.path = null
kafka2-1     | [2025-01-02 21:09:47,921] INFO RemoteLogManagerConfig values:
kafka1-1     |  remote.log.metadata.manager.impl.prefix = rlmm.config.
kafka2-1     |  log.local.retention.bytes = -2
kafka1-1     |  remote.log.metadata.manager.listener.name = null
kafka2-1     |  log.local.retention.ms = -2
kafka1-1     |  remote.log.reader.max.pending.tasks = 100
kafka2-1     |  remote.fetch.max.wait.ms = 500
kafka1-1     |  remote.log.reader.threads = 10
kafka2-1     |  remote.log.index.file.cache.total.size.bytes = 1073741824
kafka1-1     |  remote.log.storage.manager.class.name = null
kafka2-1     |  remote.log.manager.copier.thread.pool.size = 10
kafka1-1     |  remote.log.storage.manager.class.path = null
kafka2-1     |  remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
kafka1-1     |  remote.log.storage.manager.impl.prefix = rsm.config.
kafka2-1     |  remote.log.manager.copy.quota.window.num = 11
kafka1-1     |  remote.log.storage.system.enable = false
kafka2-1     |  remote.log.manager.copy.quota.window.size.seconds = 1
kafka1-1     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
kafka2-1     |  remote.log.manager.expiration.thread.pool.size = 10
kafka2-1     |  remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
kafka2-1     |  remote.log.manager.fetch.quota.window.num = 11
kafka2-1     |  remote.log.manager.fetch.quota.window.size.seconds = 1
kafka2-1     |  remote.log.manager.task.interval.ms = 30000
kafka2-1     |  remote.log.manager.task.retry.backoff.max.ms = 30000
kafka2-1     |  remote.log.manager.task.retry.backoff.ms = 500
kafka2-1     |  remote.log.manager.task.retry.jitter = 0.2
kafka2-1     |  remote.log.manager.thread.pool.size = 10
kafka2-1     |  remote.log.metadata.custom.metadata.max.bytes = 128
kafka2-1     |  remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
kafka2-1     |  remote.log.metadata.manager.class.path = null
kafka2-1     |  remote.log.metadata.manager.impl.prefix = rlmm.config.
kafka2-1     |  remote.log.metadata.manager.listener.name = null
kafka2-1     |  remote.log.reader.max.pending.tasks = 100
kafka2-1     |  remote.log.reader.threads = 10
kafka2-1     |  remote.log.storage.manager.class.name = null
kafka2-1     |  remote.log.storage.manager.class.path = null
kafka2-1     |  remote.log.storage.manager.impl.prefix = rsm.config.
kafka2-1     |  remote.log.storage.system.enable = false
kafka2-1     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
kafka1-1     | [2025-01-02 21:09:48,228] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka2-1     | [2025-01-02 21:09:48,256] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka2-1     | [2025-01-02 21:09:48,279] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka1-1     | [2025-01-02 21:09:48,306] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka1-1     | [2025-01-02 21:09:48,307] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka2-1     | [2025-01-02 21:09:48,308] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka2-1     | [2025-01-02 21:09:48,322] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka1-1     | [2025-01-02 21:09:48,327] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka2-1     | [2025-01-02 21:09:48,370] INFO [KafkaServer id=2] Rewriting /var/lib/kafka/data/meta.properties (kafka.server.KafkaServer)
kafka1-1     | [2025-01-02 21:09:48,370] INFO [KafkaServer id=1] Rewriting /var/lib/kafka/data/meta.properties (kafka.server.KafkaServer)
kafka1-1     | [2025-01-02 21:09:48,724] INFO Loading logs from log dirs ArrayBuffer(/var/lib/kafka/data) (kafka.log.LogManager)
kafka1-1     | [2025-01-02 21:09:48,745] INFO No logs found to be loaded in /var/lib/kafka/data (kafka.log.LogManager)
kafka2-1     | [2025-01-02 21:09:48,764] INFO Loading logs from log dirs ArrayBuffer(/var/lib/kafka/data) (kafka.log.LogManager)
kafka2-1     | [2025-01-02 21:09:48,777] INFO No logs found to be loaded in /var/lib/kafka/data (kafka.log.LogManager)
kafka1-1     | [2025-01-02 21:09:48,788] INFO Loaded 0 logs in 63ms (kafka.log.LogManager)
kafka1-1     | [2025-01-02 21:09:48,794] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
kafka1-1     | [2025-01-02 21:09:48,798] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
kafka2-1     | [2025-01-02 21:09:48,806] INFO Loaded 0 logs in 40ms (kafka.log.LogManager)
kafka2-1     | [2025-01-02 21:09:48,811] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
kafka2-1     | [2025-01-02 21:09:48,823] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
kafka2-1     | [2025-01-02 21:09:48,968] INFO Starting the log cleaner (kafka.log.LogCleaner)
kafka1-1     | [2025-01-02 21:09:48,968] INFO Starting the log cleaner (kafka.log.LogCleaner)
kafka1-1     | [2025-01-02 21:09:54,491] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
kafka2-1     | [2025-01-02 21:09:54,491] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
kafka1-1     | [2025-01-02 21:09:54,621] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
kafka2-1     | [2025-01-02 21:09:54,627] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
kafka2-1     | [2025-01-02 21:09:54,780] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)
kafka1-1     | [2025-01-02 21:09:54,853] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)
kafka2-1     | [2025-01-02 21:09:55,078] INFO [zk-broker-2-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
kafka1-1     | [2025-01-02 21:09:55,083] INFO [zk-broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
kafka2-1     | [2025-01-02 21:09:56,436] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
kafka1-1     | [2025-01-02 21:09:56,493] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
kafka2-1     | [2025-01-02 21:09:56,531] INFO [SocketServer listenerType=ZK_BROKER, nodeId=2] Created data-plane 
acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
kafka1-1     | [2025-01-02 21:09:56,569] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane 
acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
kafka2-1     | [2025-01-02 21:09:56,607] INFO [zk-broker-2-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
kafka1-1     | [2025-01-02 21:09:56,624] INFO [zk-broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
kafka1-1     | [2025-01-02 21:09:56,711] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka2-1     | [2025-01-02 21:09:56,711] INFO [ExpirationReaper-2-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka1-1     | [2025-01-02 21:09:56,712] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka1-1     | [2025-01-02 21:09:56,719] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka1-1     | [2025-01-02 21:09:56,726] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka2-1     | [2025-01-02 21:09:56,730] INFO [ExpirationReaper-2-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka2-1     | [2025-01-02 21:09:56,734] INFO [ExpirationReaper-2-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka2-1     | [2025-01-02 21:09:56,743] INFO [ExpirationReaper-2-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka1-1     | [2025-01-02 21:09:56,744] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka2-1     | [2025-01-02 21:09:56,750] INFO [ExpirationReaper-2-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka1-1     | [2025-01-02 21:09:56,809] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
kafka1-1     | [2025-01-02 21:09:56,813] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)
kafka2-1     | [2025-01-02 21:09:56,820] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
kafka2-1     | [2025-01-02 21:09:56,824] INFO [AddPartitionsToTxnSenderThread-2]: Starting (kafka.server.AddPartitionsToTxnManager)
kafka1-1     | [2025-01-02 21:09:56,917] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
kafka2-1     | [2025-01-02 21:09:56,956] INFO Creating /brokers/ids/2 (is it secure? false) (kafka.zk.KafkaZkClient)
kafka1-1     | [2025-01-02 21:09:57,071] INFO Stat of the created znode at /brokers/ids/1 is: 51,51,1735852196970,1735852196970,1,0,0,72057607243366402,196,0,51
kafka1-1     |  (kafka.zk.KafkaZkClient)
kafka1-1     | [2025-01-02 21:09:57,074] INFO Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT://kafka1:9092, czxid (broker epoch): 51 (kafka.zk.KafkaZkClient)
kafka2-1     | [2025-01-02 21:09:57,076] INFO Stat of the created znode at /brokers/ids/2 is: 52,52,1735852196980,1735852196980,1,0,0,72057607243366403,196,0,52
kafka2-1     |  (kafka.zk.KafkaZkClient)
kafka2-1     | [2025-01-02 21:09:57,078] INFO Registered broker 2 at path /brokers/ids/2 with addresses: PLAINTEXT://kafka2:9093, czxid (broker epoch): 52 (kafka.zk.KafkaZkClient)
kafka2-1     | [2025-01-02 21:09:57,203] INFO [ControllerEventThread controllerId=2] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
kafka1-1     | [2025-01-02 21:09:57,204] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
kafka2-1     | [2025-01-02 21:09:57,234] INFO [ExpirationReaper-2-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka1-1     | [2025-01-02 21:09:57,236] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka2-1     | [2025-01-02 21:09:57,359] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
kafka2-1     | [2025-01-02 21:09:57,465] INFO [ExpirationReaper-2-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka2-1     | [2025-01-02 21:09:57,471] INFO [ExpirationReaper-2-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka1-1     | [2025-01-02 21:09:57,472] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka1-1     | [2025-01-02 21:09:57,473] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka2-1     | [2025-01-02 21:09:57,539] INFO [GroupCoordinator 2]: Starting up. (kafka.coordinator.group.GroupCoordinator)
kafka1-1     | [2025-01-02 21:09:57,551] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
kafka2-1     | [2025-01-02 21:09:57,573] INFO [Controller id=2] 2 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:57,583] INFO [GroupCoordinator 2]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
kafka1-1     | [2025-01-02 21:09:57,591] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
kafka2-1     | [2025-01-02 21:09:57,594] INFO [Controller id=2] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map()) (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:57,647] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)
kafka1-1     | [2025-01-02 21:09:57,648] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)
kafka1-1     | [2025-01-02 21:09:57,667] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
kafka2-1     | [2025-01-02 21:09:57,676] INFO [TransactionCoordinator id=2] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
kafka2-1     | [2025-01-02 21:09:57,750] INFO [TxnMarkerSenderThread-2]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
kafka2-1     | [2025-01-02 21:09:57,751] INFO [TransactionCoordinator id=2] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
kafka1-1     | [2025-01-02 21:09:57,755] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
kafka1-1     | [2025-01-02 21:09:57,762] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
kafka1-1     | [2025-01-02 21:09:57,812] DEBUG [Controller id=1] Broker 2 was elected as controller instead of broker 1 (kafka.controller.KafkaController)
kafka1-1     | org.apache.kafka.common.errors.ControllerMovedException: Controller moved to another broker. Aborting controller startup procedure
kafka1-1     | [2025-01-02 21:09:57,827] INFO [MetadataCache brokerId=1] Updated cache from existing None to latest Features(metadataVersion=3.8-IV0, finalizedFeatures={}, finalizedFeaturesEpoch=0). (kafka.server.metadata.ZkMetadataCache)
kafka2-1     | [2025-01-02 21:09:57,842] INFO [MetadataCache brokerId=2] Updated cache from existing None to latest Features(metadataVersion=3.8-IV0, finalizedFeatures={}, finalizedFeaturesEpoch=0). (kafka.server.metadata.ZkMetadataCache)
kafka2-1     | [2025-01-02 21:09:57,842] INFO [Controller id=2] Registering handlers (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:57,852] INFO [Controller id=2] Deleting log dir event notifications (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:57,867] INFO [Controller id=2] Deleting isr change notifications (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:57,872] INFO [Controller id=2] Initializing controller context (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:57,935] INFO [Controller id=2] Initialized broker epochs cache: HashMap(1 -> 51, 2 -> 52) (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:57,963] DEBUG [Controller id=2] Register BrokerModifications handler for Set(1, 
2) (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:57,971] INFO [ExpirationReaper-2-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka2-1     | [2025-01-02 21:09:57,985] DEBUG [Channel manager on controller 2]: Controller 2 trying to connect 
to broker 1 (kafka.controller.ControllerChannelManager)
kafka1-1     | [2025-01-02 21:09:57,991] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka2-1     | [2025-01-02 21:09:57,997] DEBUG [Channel manager on controller 2]: Controller 2 trying to connect 
to broker 2 (kafka.controller.ControllerChannelManager)
kafka2-1     | [2025-01-02 21:09:58,019] INFO [RequestSendThread controllerId=2] Starting (kafka.controller.RequestSendThread)
kafka2-1     | [2025-01-02 21:09:58,020] INFO [RequestSendThread controllerId=2] Starting (kafka.controller.RequestSendThread)
kafka2-1     | [2025-01-02 21:09:58,025] INFO [Controller id=2] Currently active brokers in the cluster: Set(1, 2) (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,033] INFO [Controller id=2] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,034] INFO [Controller id=2] Current list of topics in the cluster: HashSet() 
(kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,035] INFO [Controller id=2] Fetching topic deletions in progress (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,187] INFO [Controller id=2] List of topics to be deleted:  (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,188] INFO [Controller id=2] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,189] INFO [Controller id=2] Initializing topic deletion manager (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,190] INFO [Topic Deletion Manager 2] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager)
kafka2-1     | [2025-01-02 21:09:58,280] INFO [Controller id=2] Sending update metadata request (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,363] INFO [Controller id=2 epoch=1] Sending UpdateMetadata request to brokers HashSet(1, 2) for 0 partitions (state.change.logger)
kafka1-1     | [2025-01-02 21:09:58,425] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
kafka2-1     | [2025-01-02 21:09:58,530] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
kafka2-1     | [2025-01-02 21:09:58,591] INFO [ReplicaStateMachine controllerId=2] Initializing replica state (kafka.controller.ZkReplicaStateMachine)
kafka2-1     | [2025-01-02 21:09:58,620] INFO [ReplicaStateMachine controllerId=2] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine)
kafka2-1     | [2025-01-02 21:09:58,676] INFO [ReplicaStateMachine controllerId=2] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine)
kafka2-1     | [2025-01-02 21:09:58,678] DEBUG [ReplicaStateMachine controllerId=2] Started replica state machine with initial state -> HashMap() (kafka.controller.ZkReplicaStateMachine)
kafka2-1     | [2025-01-02 21:09:58,679] INFO [PartitionStateMachine controllerId=2] Initializing partition state (kafka.controller.ZkPartitionStateMachine)
kafka2-1     | [2025-01-02 21:09:58,711] INFO [PartitionStateMachine controllerId=2] Triggering online partition 
state changes (kafka.controller.ZkPartitionStateMachine)
kafka1-1     | [2025-01-02 21:09:58,718] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
kafka2-1     | [2025-01-02 21:09:58,720] DEBUG [PartitionStateMachine controllerId=2] Started partition state machine with initial state -> HashMap() (kafka.controller.ZkPartitionStateMachine)
kafka2-1     | [2025-01-02 21:09:58,721] INFO [Controller id=2] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,729] INFO [Controller id=2, targetBrokerId=2] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
kafka2-1     | [2025-01-02 21:09:58,729] INFO [Controller id=2, targetBrokerId=1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
kafka2-1     | [2025-01-02 21:09:58,731] INFO [SocketServer listenerType=ZK_BROKER, nodeId=2] Enabling request processing. (kafka.network.SocketServer)
kafka2-1     | [2025-01-02 21:09:58,742] WARN [Controller id=2, targetBrokerId=2] Connection to node 2 (kafka2/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)     
kafka1-1     | [2025-01-02 21:09:58,741] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
kafka2-1     | [2025-01-02 21:09:58,746] WARN [Controller id=2, targetBrokerId=1] Connection to node 1 (kafka1/172.19.0.3:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)     
kafka2-1     | [2025-01-02 21:09:58,746] INFO [Controller id=2] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,749] WARN [RequestSendThread controllerId=2] Controller 2's connection to broker kafka2:9093 (id: 2 rack: null) was unsuccessful (kafka.controller.RequestSendThread)
kafka2-1     | java.io.IOException: Connection to kafka2:9093 (id: 2 rack: null) failed.
kafka2-1     |  at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
kafka2-1     |  at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:299)
kafka2-1     |  at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:252)
kafka2-1     |  at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
kafka2-1     | [2025-01-02 21:09:58,757] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)
kafka2-1     | [2025-01-02 21:09:58,749] WARN [RequestSendThread controllerId=2] Controller 2's connection to broker kafka1:9092 (id: 1 rack: null) was unsuccessful (kafka.controller.RequestSendThread)
kafka2-1     | java.io.IOException: Connection to kafka1:9092 (id: 1 rack: null) failed.
kafka2-1     |  at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
kafka2-1     |  at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:299)
kafka2-1     |  at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:252)
kafka2-1     |  at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
kafka2-1     | [2025-01-02 21:09:58,749] INFO [Controller id=2] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,758] INFO [Controller id=2, targetBrokerId=1] Client requested connection close from node 1 (org.apache.kafka.clients.NetworkClient)
kafka2-1     | [2025-01-02 21:09:58,757] INFO [Controller id=2, targetBrokerId=2] Client requested connection close from node 2 (org.apache.kafka.clients.NetworkClient)
kafka2-1     | [2025-01-02 21:09:58,759] INFO [Controller id=2] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,760] INFO [Controller id=2] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,763] INFO [Controller id=2] Starting replica leader election (PREFERRED) for 
partitions  triggered by ZkTriggered (kafka.controller.KafkaController)
kafka1-1     | [2025-01-02 21:09:58,761] INFO [KafkaServer id=1] Start processing authorizer futures (kafka.server.KafkaServer)
kafka1-1     | [2025-01-02 21:09:58,772] INFO [KafkaServer id=1] End processing authorizer futures (kafka.server.KafkaServer)
kafka1-1     | [2025-01-02 21:09:58,775] INFO [KafkaServer id=1] Start processing enable request processing future (kafka.server.KafkaServer)
kafka1-1     | [2025-01-02 21:09:58,776] INFO [KafkaServer id=1] End processing enable request processing future 
(kafka.server.KafkaServer)
kafka2-1     | [2025-01-02 21:09:58,786] INFO [KafkaServer id=2] Start processing authorizer futures (kafka.server.KafkaServer)
kafka2-1     | [2025-01-02 21:09:58,787] INFO [KafkaServer id=2] End processing authorizer futures (kafka.server.KafkaServer)
kafka2-1     | [2025-01-02 21:09:58,794] INFO [KafkaServer id=2] Start processing enable request processing future (kafka.server.KafkaServer)
kafka2-1     | [2025-01-02 21:09:58,799] INFO [KafkaServer id=2] End processing enable request processing future 
(kafka.server.KafkaServer)
kafka1-1     | [2025-01-02 21:09:58,800] INFO Kafka version: 7.8.0-ccs (org.apache.kafka.common.utils.AppInfoParser)
kafka1-1     | [2025-01-02 21:09:58,801] INFO Kafka commitId: cc7168da1fddfcfde48b42031adde57bb5bcf529 (org.apache.kafka.common.utils.AppInfoParser)
kafka1-1     | [2025-01-02 21:09:58,801] INFO Kafka startTimeMs: 1735852198776 (org.apache.kafka.common.utils.AppInfoParser)
kafka1-1     | [2025-01-02 21:09:58,806] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
kafka2-1     | [2025-01-02 21:09:58,820] INFO Kafka version: 7.8.0-ccs (org.apache.kafka.common.utils.AppInfoParser)
kafka2-1     | [2025-01-02 21:09:58,820] INFO Kafka commitId: cc7168da1fddfcfde48b42031adde57bb5bcf529 (org.apache.kafka.common.utils.AppInfoParser)
kafka2-1     | [2025-01-02 21:09:58,820] INFO Kafka startTimeMs: 1735852198799 (org.apache.kafka.common.utils.AppInfoParser)
kafka2-1     | [2025-01-02 21:09:58,825] INFO [KafkaServer id=2] started (kafka.server.KafkaServer)
kafka2-1     | [2025-01-02 21:09:58,838] INFO [Controller id=2] Starting the controller scheduler (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:09:58,869] INFO [RequestSendThread controllerId=2] Controller 2 connected to kafka1:9092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
kafka2-1     | [2025-01-02 21:09:58,870] INFO [RequestSendThread controllerId=2] Controller 2 connected to kafka2:9093 (id: 2 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
kafka2-1     | [2025-01-02 21:09:59,004] INFO [zk-broker-2-to-controller-forwarding-channel-manager]: Recorded new ZK controller, from now on will use node kafka2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
kafka2-1     | [2025-01-02 21:09:59,013] INFO [zk-broker-2-to-controller-alter-partition-channel-manager]: Recorded new ZK controller, from now on will use node kafka2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
kafka2-1     | [2025-01-02 21:09:59,029] TRACE [Controller id=2 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 0 sent to broker kafka2:9093 (id: 2 rack: null) (state.change.logger)
kafka1-1     | [2025-01-02 21:09:59,114] INFO [zk-broker-1-to-controller-alter-partition-channel-manager]: Recorded new ZK controller, from now on will use node kafka2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
kafka1-1     | [2025-01-02 21:09:59,114] INFO [zk-broker-1-to-controller-forwarding-channel-manager]: Recorded new ZK controller, from now on will use node kafka2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
kafka2-1     | [2025-01-02 21:09:59,130] TRACE [Controller id=2 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 0 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
kafka2-1     | [2025-01-02 21:10:03,841] INFO [Controller id=2] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka2-1     | [2025-01-02 21:10:03,842] TRACE [Controller id=2] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka1-1     | [2025-01-02 21:10:16,711] WARN [SocketServer listenerType=ZK_BROKER, nodeId=1] Unexpected error from /172.19.0.1 (channelId=172.19.0.3:9092-172.19.0.1:55334-0); closing connection (org.apache.kafka.common.network.Selector)
kafka1-1     | org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 1195725856 larger than 104857600)
kafka1-1     |  at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:94)
kafka1-1     |  at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:462)
kafka1-1     |  at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:412)
kafka1-1     |  at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:678)
kafka1-1     |  at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:580)
kafka1-1     |  at org.apache.kafka.common.network.Selector.poll(Selector.java:485)
kafka1-1     |  at kafka.network.Processor.poll(SocketServer.scala:1111)
kafka1-1     |  at kafka.network.Processor.run(SocketServer.scala:1015)
kafka1-1     |  at java.base/java.lang.Thread.run(Unknown Source)
kafka1-1     | [2025-01-02 21:10:17,980] WARN [SocketServer listenerType=ZK_BROKER, nodeId=1] Unexpected error from /172.19.0.1 (channelId=172.19.0.3:9092-172.19.0.1:55342-0); closing connection (org.apache.kafka.common.network.Selector)
kafka1-1     | org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 1195725856 larger than 104857600)
kafka1-1     |  at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:94)
kafka1-1     |  at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:462)
kafka1-1     |  at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:412)
kafka1-1     |  at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:678)
kafka1-1     |  at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:580)
kafka1-1     |  at org.apache.kafka.common.network.Selector.poll(Selector.java:485)
kafka1-1     |  at kafka.network.Processor.poll(SocketServer.scala:1111)
kafka1-1     |  at kafka.network.Processor.run(SocketServer.scala:1015)
kafka1-1     |  at java.base/java.lang.Thread.run(Unknown Source)
kafka1-1     | [2025-01-02 21:10:23,647] WARN [SocketServer listenerType=ZK_BROKER, nodeId=1] Unexpected error from /172.19.0.1 (channelId=172.19.0.3:9092-172.19.0.1:55356-1); closing connection (org.apache.kafka.common.network.Selector)
kafka1-1     | org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 1195725856 larger than 104857600)
kafka1-1     |  at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:94)
kafka1-1     |  at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:462)
kafka1-1     |  at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:412)
kafka1-1     |  at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:678)
kafka1-1     |  at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:580)
kafka1-1     |  at org.apache.kafka.common.network.Selector.poll(Selector.java:485)
kafka1-1     |  at kafka.network.Processor.poll(SocketServer.scala:1111)
kafka1-1     |  at kafka.network.Processor.run(SocketServer.scala:1015)
kafka1-1     |  at java.base/java.lang.Thread.run(Unknown Source)
kafka1-1     | [2025-01-02 21:10:54,292] WARN [SocketServer listenerType=ZK_BROKER, nodeId=1] Unexpected error from /172.19.0.1 (channelId=172.19.0.3:9092-172.19.0.1:36948-1); closing connection (org.apache.kafka.common.network.Selector)
kafka1-1     | org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 1195725856 larger than 104857600)
kafka1-1     |  at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:94)
kafka1-1     |  at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:462)
kafka1-1     |  at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:412)
kafka1-1     |  at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:678)
kafka1-1     |  at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:580)
kafka1-1     |  at org.apache.kafka.common.network.Selector.poll(Selector.java:485)
kafka1-1     |  at kafka.network.Processor.poll(SocketServer.scala:1111)
kafka1-1     |  at kafka.network.Processor.run(SocketServer.scala:1015)
kafka1-1     |  at java.base/java.lang.Thread.run(Unknown Source)